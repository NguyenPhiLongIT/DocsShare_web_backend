"""
SEMANTIC SEARCH MODULE
=====================
QUAN TR·ªåNG: Module n√†y ch·ªâ S·ª¨ D·ª§NG model, KH√îNG THAY ƒê·ªîI model.

Model ƒë∆∞·ª£c load t·ª´ semantic_search_model.pkl v·ªõi:
- doc_full_vecs: Vectors c·ªßa documents (ƒë√£ normalize b·ªüi model khi t·∫°o)
- df_kw: Keywords dataset cho t·ª´ng topic
- doc_meta: Metadata c·ªßa documents

INPUT FORMAT (theo thi·∫øt k·∫ø model):
- Query: embed v·ªõi prefix "query:" 
- Keywords/Passages: embed v·ªõi prefix "passage:"
- normalize_embeddings=True trong model.encode()

OUTPUT FORMAT:
- Tr·∫£ v·ªÅ danh s√°ch topics v·ªõi documents, m·ªói item c√≥ similarity score
- Format ph√π h·ª£p v·ªõi SemanticSearchService.java DTO

KH√îNG THAY ƒê·ªîI:
- C√°ch normalize vectors (model ƒë√£ normalize)
- Input format (query:/passage: prefix)
- Output format (DTO structure)
- Embedding model settings
"""

from flask import Flask, request, jsonify
from urllib.parse import unquote
import pickle
import os
import sys
from collections import defaultdict

# Import numpy tr∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o module ƒë∆∞·ª£c load ƒë·∫ßy ƒë·ªß
import numpy as np
# Force import numpy._core ƒë·ªÉ tr√°nh l·ªói khi unpickle
try:
    import numpy._core.numeric as _  # noqa: F401
except ImportError:
    pass

import faiss
from sentence_transformers import SentenceTransformer


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# L√™n m·ªôt c·∫•p t·ª´ semantic/ ƒë·ªÉ ƒë·∫øn flask_server/
FLASK_SERVER_DIR = os.path.dirname(BASE_DIR)

# File semantic_search_model.pkl n·∫±m trong th∆∞ m·ª•c models
MODEL_PKL_PATH = os.path.join(FLASK_SERVER_DIR, "models", "semantic_search_model.pkl")

print("Loading semantic search model from:", MODEL_PKL_PATH)

# Workaround cho numpy compatibility issue
# ƒê·∫£m b·∫£o t·∫•t c·∫£ numpy submodules ƒë∆∞·ª£c import tr∆∞·ªõc khi unpickle
def _ensure_numpy_modules():
    """ƒê·∫£m b·∫£o c√°c numpy modules c·∫ßn thi·∫øt ƒë∆∞·ª£c import"""
    try:
        # Import c√°c modules numpy c√≥ th·ªÉ c·∫ßn
        import numpy._core
        import numpy._core.multiarray
        import numpy._core.numeric
        import numpy._core.umath
    except (ImportError, AttributeError):
        # N·∫øu kh√¥ng c√≥ _core (numpy c≈©), b·ªè qua
        pass

_ensure_numpy_modules()

try:
    with open(MODEL_PKL_PATH, "rb") as f:
        state = pickle.load(f)
except (ModuleNotFoundError, AttributeError) as e:
    # L·ªói do numpy version mismatch
    print(f"‚ùå Error loading model: {e}")
    print("üí° This is likely due to numpy version incompatibility.")
    print(f"   Current numpy version: {np.__version__}")
    print("\nüîß Solutions:")
    print("   1. Try reinstalling numpy:")
    print("      pip uninstall numpy")
    print("      pip install 'numpy<2.0'")
    print("   2. Or try with specific version that matches the model:")
    print("      pip install numpy==1.24.3  # Example")
    print("   3. Re-pickle the model with current numpy version")
    raise RuntimeError(
        f"Cannot load model due to numpy compatibility issue: {e}\n"
        "Please try reinstalling numpy or re-pickle the model."
    ) from e


config = state["config"]
EMBEDDING_MODEL_NAME = config["embedding_model_name"]
DEFAULT_SIM_THRESHOLD = float(config.get("sim_threshold_default", 0.2))  # Gi·∫£m threshold m·∫∑c ƒë·ªãnh t·ª´ 0.3 xu·ªëng 0.2
DEFAULT_TOP_K_TOPICS = int(config.get("top_k_topics_default", 5))  # TƒÉng s·ªë topics t√¨m t·ª´ 3 l√™n 5
DEFAULT_TOP_K_DOCS = int(config.get("top_k_docs_default", 10))

print(f"‚öôÔ∏è  Default settings:")
print(f"   - Sim threshold: {DEFAULT_SIM_THRESHOLD}")
print(f"   - Top K topics: {DEFAULT_TOP_K_TOPICS}")
print(f"   - Top K docs per topic: {DEFAULT_TOP_K_DOCS}")

print("Embedding model:", EMBEDDING_MODEL_NAME)
print("üí° Model s·∫Ω ƒë∆∞·ª£c load lazy (ch·ªâ khi c√≥ request ƒë·∫ßu ti√™n)")

# Lazy loading: Model s·∫Ω ch·ªâ ƒë∆∞·ª£c load khi c√≥ request ƒë·∫ßu ti√™n
_model = None

def get_model():
    """Lazy load model - ch·ªâ load khi c·∫ßn thi·∫øt"""
    global _model
    if _model is None:
        print("üîÑ ƒêang load embedding model (l·∫ßn ƒë·∫ßu ti√™n)...")
        _model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng")
    return _model

def embed_query(texts):
    """Embed query text - model s·∫Ω ƒë∆∞·ª£c load t·ª± ƒë·ªông n·∫øu ch∆∞a c√≥"""
    model = get_model()
    texts = [f"query: {t}" for t in texts]
    return model.encode(texts, normalize_embeddings=True).astype("float32")

def embed_passage(texts):
    """Embed passage text - model s·∫Ω ƒë∆∞·ª£c load t·ª± ƒë·ªông n·∫øu ch∆∞a c√≥"""
    model = get_model()
    texts = [f"passage: {t}" for t in texts]
    return model.encode(texts, normalize_embeddings=True).astype("float32")


# ====== KH√îI PH·ª§C C√ÅC BI·∫æN T·ª™ .PKL ======
# (c·∫•u tr√∫c ƒë√∫ng nh∆∞ notebook ƒë√£ save)
config          = state["config"]
df_kw           = state["df_kw"]

# FAISS keyword index (deserialize t·ª´ bytes)
kw_index_bytes  = state["kw_index_bytes"]
kw_index        = faiss.deserialize_index(kw_index_bytes)

# N-gram vectors theo category
doc_vecs        = state["doc_vecs"].astype("float32")    # (n_chunks, dim)
doc_ids_arr     = state["doc_ids_arr"].astype("int64")   # doc_id cho t·ª´ng chunk
cat_ids_arr     = state["cat_ids_arr"].astype("int64")   # category_id cho t·ª´ng chunk

# Vector full doc (summary / description)
doc_full_vecs   = state["doc_full_vecs"].astype("float32")  # (n_docs, dim)
doc_full_ids    = state["doc_full_ids"].astype("int64")
doc_meta        = state["doc_meta"]   # {category_id -> list[meta]}

# Map doc_id -> full vector (ƒë·ªÉ d√πng nhanh trong search)
doc_full_map = {
    int(doc_full_ids[i]): doc_full_vecs[i]
    for i in range(len(doc_full_ids))
}

print("Model loaded:")
print("   #keywords:", len(df_kw))
print("   #chunks (n-grams):", doc_vecs.shape[0])
print("   #docs (full):", len(doc_full_ids))


# Map doc_id -> full vector (summary vector)
# QUAN TR·ªåNG: Kh√¥ng normalize l·∫°i - vectors t·ª´ model ƒë√£ ƒë∆∞·ª£c normalize khi t·∫°o
# doc_full_vecs ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi normalize_embeddings=True trong model.encode()
# N·∫øu normalize l·∫°i s·∫Ω l√†m sai k·∫øt qu·∫£ similarity
doc_summary_vecs = {}
for i in range(len(doc_full_ids)):
    doc_id = int(doc_full_ids[i])
    doc_vec = doc_full_vecs[i].astype("float32")
    # Ki·ªÉm tra xem vector ƒë√£ ƒë∆∞·ª£c normalize ch∆∞a (norm ‚âà 1.0)
    vec_norm = np.linalg.norm(doc_vec)
    if abs(vec_norm - 1.0) > 0.01:  # N·∫øu norm kh√°c 1.0 nhi·ªÅu, c√≥ th·ªÉ ch∆∞a normalize
        print(f"‚ö†Ô∏è  Warning: Document {doc_id} vector norm = {vec_norm:.4f} (expected ~1.0)")
    doc_summary_vecs[doc_id] = doc_vec

print("Model loaded:")
print("   #keywords:", len(df_kw))
print("   #docs with summaries:", len(doc_summary_vecs))

# ====== X√ÇY D·ª∞NG T·ª™ ƒêI·ªÇN KEYWORD THEO CH·ª¶ ƒê·ªÄ ======
# Lazy loading: Ch·ªâ build topic vectors khi c·∫ßn thi·∫øt (l·∫ßn ƒë·∫ßu search)
_topic_vectors = None
_topic_names = None
_topic_index = None
_topic_ids_list = None

# File cache cho topic vectors
TOPIC_VECTORS_CACHE_PATH = os.path.join(FLASK_SERVER_DIR, "models", "topic_vectors_cache.pkl")

def build_topic_vectors():
    """Lazy build topic vectors - ch·ªâ build khi c·∫ßn thi·∫øt, cache v√†o file"""
    global _topic_vectors, _topic_names, _topic_index, _topic_ids_list
    
    if _topic_vectors is not None:
        return _topic_vectors, _topic_names, _topic_index, _topic_ids_list
    
    # Th·ª≠ load t·ª´ cache tr∆∞·ªõc
    if os.path.exists(TOPIC_VECTORS_CACHE_PATH):
        try:
            print("üìÇ ƒêang load topic vectors t·ª´ cache...")
            with open(TOPIC_VECTORS_CACHE_PATH, "rb") as f:
                cache_data = pickle.load(f)
                _topic_vectors = cache_data["topic_vectors"]
                _topic_names = cache_data["topic_names"]
                topic_ids_list = cache_data["topic_ids_list"]
                
                # Rebuild FAISS index t·ª´ cached vectors
                topic_vecs_array = np.array([_topic_vectors[tid] for tid in topic_ids_list]).astype("float32")
                _topic_index = faiss.IndexFlatIP(topic_vecs_array.shape[1])
                _topic_index.add(topic_vecs_array)
                _topic_ids_list = topic_ids_list
                
                print(f"   ‚úÖ ƒê√£ load {len(_topic_vectors)} topic vectors t·ª´ cache")
                return _topic_vectors, _topic_names, _topic_index, _topic_ids_list
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ load cache: {e}, s·∫Ω build l·∫°i...")
    
    # N·∫øu kh√¥ng c√≥ cache, build m·ªõi
    print("üîÑ ƒêang build topic vectors t·ª´ keywords (l·∫ßn ƒë·∫ßu ti√™n)...")
    print("   ‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")
    
    # M·ªói ch·ªß ƒë·ªÅ (category) c√≥ m·ªôt t·∫≠p h·ª£p c√°c keyword v·ªõi vector t∆∞∆°ng ·ª©ng
    topic_keyword_dict = defaultdict(list)  # {topic_id: [(keyword, vector), ...]}

    # Batch embed keywords ƒë·ªÉ tƒÉng t·ªëc (embed nhi·ªÅu keywords c√πng l√∫c)
    all_keywords = []
    keyword_meta = []  # L∆∞u metadata ƒë·ªÉ map l·∫°i
    
    for idx, row in df_kw.iterrows():
        topic_id = int(row["category_id"])
        keyword = str(row["keyword"])
        all_keywords.append(keyword)
        keyword_meta.append({
            "topic_id": topic_id,
            "category_name": str(row.get("category_name", ""))
        })
    
    # Batch embed t·∫•t c·∫£ keywords c√πng l√∫c (nhanh h∆°n nhi·ªÅu)
    print(f"   üìù ƒêang embed {len(all_keywords)} keywords...")
    all_vectors = embed_passage(all_keywords)
    
    # Map vectors v·ªÅ topics
    for i, (keyword, vector, meta) in enumerate(zip(all_keywords, all_vectors, keyword_meta)):
        topic_id = meta["topic_id"]
        topic_keyword_dict[topic_id].append({
            "keyword": keyword,
            "vector": vector,
            "category_name": meta["category_name"]
        })
    
    print(f"   ‚úÖ ƒê√£ embed xong, ƒëang t√≠nh topic vectors...")

    # T·∫°o vector ƒë·∫°i di·ªán cho m·ªói topic (trung b√¨nh c√°c keyword vectors)
    # M·ªói topic c√≥ m·ªôt t·∫≠p h·ª£p keywords v·ªõi vector t∆∞∆°ng ·ª©ng
    topic_vectors = {}
    topic_names = {}
    topic_keyword_vectors = {}  # L∆∞u t·ª´ ƒëi·ªÉn keyword vectors cho m·ªói topic
    
    for topic_id, keywords in topic_keyword_dict.items():
        if keywords:
            # L·∫•y t√™n category t·ª´ keyword ƒë·∫ßu ti√™n
            topic_names[topic_id] = keywords[0]["category_name"]
            
            # T√≠nh vector trung b√¨nh c·ªßa t·∫•t c·∫£ keywords trong topic
            # QUAN TR·ªåNG: Vectors t·ª´ embed_passage() ƒë√£ ƒë∆∞·ª£c normalize (normalize_embeddings=True)
            # Kh√¥ng normalize l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n v·ªõi model
            kw_vectors = []
            keyword_dict = {}  # T·ª´ ƒëi·ªÉn keyword -> vector cho topic n√†y
            
            for kw in keywords:
                kw_vec = kw["vector"].astype("float32")
                # Vector ƒë√£ ƒë∆∞·ª£c normalize trong embed_passage(), kh√¥ng normalize l·∫°i
                kw_vectors.append(kw_vec)
                keyword_dict[kw["keyword"]] = kw_vec
            
            # L∆∞u t·ª´ ƒëi·ªÉn keyword vectors cho topic
            topic_keyword_vectors[topic_id] = keyword_dict
            
            # T√≠nh vector trung b√¨nh (vectors ƒë√£ normalize t·ª´ model)
            kw_vectors_array = np.array(kw_vectors)
            topic_vector = np.mean(kw_vectors_array, axis=0).astype("float32")
            
            # QUAN TR·ªåNG: Normalize l·∫°i vector trung b√¨nh
            # Vector trung b√¨nh c·ªßa c√°c normalized vectors kh√¥ng nh·∫•t thi·∫øt l√† normalized
            # C·∫ßn normalize ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n khi t√≠nh cosine similarity
            vec_norm = np.linalg.norm(topic_vector)
            if vec_norm > 0:
                topic_vector = topic_vector / vec_norm
            topic_vectors[topic_id] = topic_vector.astype("float32")

    print(f"   ‚úÖ ƒê√£ build {len(topic_vectors)} topic vectors")

    # T·∫°o FAISS index cho topic vectors ƒë·ªÉ t√¨m topic ph√π h·ª£p nhanh
    topic_ids_list = list(topic_vectors.keys())
    topic_vecs_array = np.array([topic_vectors[tid] for tid in topic_ids_list]).astype("float32")
    topic_index = faiss.IndexFlatIP(topic_vecs_array.shape[1])  # Inner Product for cosine similarity
    topic_index.add(topic_vecs_array)
    
    # Cache v√†o file ƒë·ªÉ l·∫ßn sau kh√¥ng ph·∫£i build l·∫°i
    try:
        print("üíæ ƒêang l∆∞u topic vectors v√†o cache...")
        cache_data = {
            "topic_vectors": topic_vectors,
            "topic_names": topic_names,
            "topic_ids_list": topic_ids_list
        }
        with open(TOPIC_VECTORS_CACHE_PATH, "wb") as f:
            pickle.dump(cache_data, f)
        print("   ‚úÖ ƒê√£ l∆∞u cache th√†nh c√¥ng")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u cache: {e}")
    
    # Cache trong memory
    _topic_vectors = topic_vectors
    _topic_names = topic_names
    _topic_index = topic_index
    _topic_ids_list = topic_ids_list
    
    return topic_vectors, topic_names, topic_index, topic_ids_list


# ====== H√ÄM SEARCH CORE M·ªöI ======
def search_core(query: str,
                top_k_topics: int = None,          # s·∫Ω d√πng l√†m top_k_keywords
                top_k_docs_per_topic: int = None,  # s·ªë doc tr·∫£ v·ªÅ
                sim_threshold: float = None):
    """
    Smart Search V2 ‚Äì b√°m theo logic search_once() trong notebook.

    - B∆∞·ªõc 1: ƒêo√°n category b·∫±ng keyword g·∫ßn nh·∫•t (kw_index).
    - B∆∞·ªõc 2: Ch·ªâ l·∫•y n-gram c·ªßa category ƒë√≥, t√≠nh max_sim_q theo doc.
    - B∆∞·ªõc 3: K·∫øt h·ª£p sim_full + max_sim_q -> sim_doc_dense = w_full*sim_full + w_local*max_sim_q.
    - FE v·∫´n nh·∫≠n format:
        {
          query,
          sim_threshold,
          results: [
            {
              topic_id,
              topic_name,
              topic_similarity,   # = keyword_sim
              documents: [
                { doc_id, title, summary, similarity, ... }
              ]
            }
          ]
        }
    """

    # 0) Validate + clean query
    if not query:
        return {
            "query": query,
            "results": [],
            "error": "EMPTY_QUERY"
        }

    query_clean = str(query).strip()
    if not query_clean:
        return {
            "query": query,
            "results": [],
            "error": "EMPTY_QUERY"
        }

    # C√°c tham s·ªë m·∫∑c ƒë·ªãnh t·ª´ config notebook
    if sim_threshold is None:
        sim_threshold = float(config.get("sim_threshold_default", 0.3))
    if top_k_docs_per_topic is None:
        top_k_docs = DEFAULT_TOP_K_DOCS
    else:
        top_k_docs = int(top_k_docs_per_topic)
    # notebook d√πng top_k_keywords, map t·ª´ top_k_topics
    if top_k_topics is None:
        top_k_keywords = 5
    else:
        top_k_keywords = int(top_k_topics)

    w_full  = float(config.get("w_full_default", 0.6))
    w_local = float(config.get("w_local_default", 0.4))

    print("\nüîç ====== SMART SEARCH (notebook logic) ======")
    print(f"   Query: '{query_clean}'")
    print(f"   sim_threshold = {sim_threshold}, top_k_keywords = {top_k_keywords}, top_k_docs = {top_k_docs}")
    print(f"   w_full = {w_full}, w_local = {w_local}")

    # 1) Embed query
    q_vec = embed_query([query_clean]).astype("float32")  # (1, dim)
    print(f"   ‚úÖ Query vector shape: {q_vec.shape}, norm={np.linalg.norm(q_vec):.4f}")

    # 2) ƒêo√°n category b·∫±ng keyword (kw_index)
    D_kw, I_kw = kw_index.search(q_vec, top_k_keywords)
    best_pos = int(I_kw[0][0])
    best_sim = float(D_kw[0][0])

    best_kw_row = df_kw.iloc[best_pos]
    cat_id   = int(best_kw_row["category_id"])
    cat_name = str(best_kw_row["category_name"])
    kw_text  = str(best_kw_row["keyword"])

    print(f"\nüéØ D·ª± ƒëo√°n category theo keyword:")
    print(f"   - category_id   : {cat_id}")
    print(f"   - category_name : {cat_name}")
    print(f"   - keyword match : \"{kw_text}\"")
    print(f"   - keyword_sim   : {best_sim:.3f}")

    # 3) L·∫•y t·∫•t c·∫£ n-gram thu·ªôc category ƒë√≥
    mask = (cat_ids_arr == cat_id)
    if not mask.any():
        print("\n‚ö† Category n√†y ch∆∞a c√≥ n-gram n√†o (kh√¥ng c√≥ t√†i li·ªáu).")
        return {
            "query": query_clean,
            "sim_threshold": sim_threshold,
            "results": [
                {
                    "topic_id": cat_id,
                    "topic_name": cat_name,
                    "keyword_match": kw_text,
                    "topic_similarity": best_sim,
                    "documents": []
                }
            ]
        }

    vecs_cat    = doc_vecs[mask].astype("float32")     # (N_chunk, dim)
    doc_ids_cat = doc_ids_arr[mask].astype("int64")    # (N_chunk,)
    print(f"   üìÑ S·ªë chunk (n-gram) trong category {cat_id}: {vecs_cat.shape[0]}")

    # 4) T√≠nh similarity query - t·ª´ng n-gram (cosine = dot v√¨ vector ƒë√£ normalize)
    sim_q = (vecs_cat @ q_vec.T).reshape(-1)           # (N_chunk,)

    # 5) G·ªôp theo doc_id, ch·ªâ gi·ªØ max_sim_q
    doc_scores: dict[int, dict] = {}
    for chunk_sim_q, doc_id_val in zip(sim_q, doc_ids_cat):
        if float(chunk_sim_q) < sim_threshold:
            continue

        doc_id_int = int(doc_id_val)
        val = float(chunk_sim_q)

        if doc_id_int not in doc_scores:
            doc_scores[doc_id_int] = {"max_sim_q": val}
        else:
            if val > doc_scores[doc_id_int]["max_sim_q"]:
                doc_scores[doc_id_int]["max_sim_q"] = val

    if not doc_scores:
        print("\n  (Kh√¥ng c√≥ doc n√†o ƒë·ªß similarity theo threshold)")
        return {
            "query": query_clean,
            "sim_threshold": sim_threshold,
            "results": [
                {
                    "topic_id": cat_id,
                    "topic_name": cat_name,
                    "keyword_match": kw_text,
                    "topic_similarity": best_sim,
                    "documents": []
                }
            ]
        }

    # 6) Th√™m sim_full v√† sim_doc_dense = w_full * sim_full + w_local * max_sim_q
    for doc_id_int, vals in doc_scores.items():
        doc_full_vec = doc_full_map.get(doc_id_int)
        if doc_full_vec is None:
            sim_full = vals["max_sim_q"]    # fallback ƒë∆°n gi·∫£n
        else:
            sim_full = float(doc_full_vec @ q_vec.T)  # cosine

        vals["sim_full"]      = sim_full
        vals["sim_doc_dense"] = w_full * sim_full + w_local * vals["max_sim_q"]

    # 7) S·∫Øp x·∫øp theo sim_doc_dense
    sorted_docs = sorted(
        doc_scores.items(),
        key=lambda x: x[1]["sim_doc_dense"],
        reverse=True
    )

    # 8) L·∫•y metadata theo category + doc_id
    meta_list = doc_meta.get(str(cat_id)) or doc_meta.get(int(cat_id), [])
    meta_by_id = {int(m["doc_id"]): m for m in meta_list}

    documents = []
    for doc_id_int, vals in sorted_docs[:top_k_docs]:
        m = meta_by_id.get(doc_id_int, {})
        documents.append({
            "doc_id": int(doc_id_int),
            "title": m.get("title", ""),
            "summary": m.get("summary", m.get("description", "")),
            # 3 lo·∫°i similarity n·∫øu sau n√†y mu·ªën debug:
            "similarity_full":   vals["sim_full"],
            "similarity_ngram":  vals["max_sim_q"],
            "similarity":        vals["sim_doc_dense"],   # FE ƒëang d√πng field n√†y
        })

    print(f"\nüìö Top {len(documents)} documents trong category {cat_id}:")
    for i, d in enumerate(documents[:5], 1):
        print(f"   {i}. [{d['doc_id']}] {d['title'][:60]}...  (sim={d['similarity']:.3f})")

    # 9) Format JSON tr·∫£ cho FE (1 topic + danh s√°ch documents)
    result = {
        "query": query_clean,
        "sim_threshold": sim_threshold,
        "results": [
            {
                "topic_id": cat_id,
                "topic_name": cat_name,
                "keyword_match": kw_text,
                "topic_similarity": best_sim,  # d√πng keyword_sim
                "documents": documents
            }
        ]
    }

    return result



# ====== FLASK API ======
app = Flask(__name__)

@app.route("/semantic-search", methods=["POST"])
def semantic_search():
    data = request.get_json(force=True) or {}
    query = data.get("query", "")
    top_k_topics = data.get("top_k_topics")
    top_k_docs_per_topic = data.get("top_k_docs_per_topic")
    sim_threshold = data.get("sim_threshold")

    res = search_core(
        query=query,
        top_k_topics=top_k_topics,
        top_k_docs_per_topic=top_k_docs_per_topic,
        sim_threshold=sim_threshold,
    )

    return jsonify(res)


# Blueprint ƒë·ªÉ register v√†o app ch√≠nh
from flask import Blueprint
semantic_bp = Blueprint('semantic', __name__)

@semantic_bp.route("/semantic/search", methods=["GET", "POST"])
def semantic_search_endpoint():
    # H·ªó tr·ª£ c·∫£ GET v√† POST
    if request.method == "GET":
        # Flask t·ª± ƒë·ªông decode URL encoding t·ª´ URLEncoder.encode() trong Java
        # Nh∆∞ng c·∫ßn ki·ªÉm tra xem c√≥ c·∫ßn unquote th√™m kh√¥ng
        query_raw = request.args.get("query", "")
        # Ki·ªÉm tra n·∫øu query c√≥ d·∫•u % (c√≥ th·ªÉ ch∆∞a ƒë∆∞·ª£c decode)
        # N·∫øu c√≥, unquote m·ªôt l·∫ßn n·ªØa (tr√°nh double decode)
        if query_raw and '%' in query_raw:
            # C√≥ th·ªÉ c·∫ßn unquote th√™m n·∫øu Flask ch∆∞a decode h·∫øt
            try:
                query = unquote(query_raw, encoding='utf-8')
                # N·∫øu sau khi unquote v·∫´n c√≤n %, c√≥ th·ªÉ l√† double encoding
                if '%' in query:
                    query = unquote(query, encoding='utf-8')
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi unquote query: {e}, d√πng query g·ªëc")
                query = query_raw
        else:
            # Flask ƒë√£ decode r·ªìi, d√πng tr·ª±c ti·∫øp
            query = query_raw
        top_k_topics = request.args.get("top_k_topics")
        top_k_docs_per_topic = request.args.get("top_k_docs_per_topic")
        sim_threshold = request.args.get("sim_threshold")
        
        if top_k_topics:
            top_k_topics = int(top_k_topics)
        if top_k_docs_per_topic:
            top_k_docs_per_topic = int(top_k_docs_per_topic)
        if sim_threshold:
            sim_threshold = float(sim_threshold)
    else:
        data = request.get_json(force=True) or {}
        query = data.get("query", "")
        top_k_topics = data.get("top_k_topics")
        top_k_docs_per_topic = data.get("top_k_docs_per_topic")
        sim_threshold = data.get("sim_threshold")

    # QUAN TR·ªåNG: Log query ƒë·ªÉ ki·ªÉm tra input v√†o model
    print(f"üîç Semantic Search Request:")
    if request.method == "GET":
        query_raw_for_log = request.args.get("query", "")
        print(f"   Raw query (from URL): '{query_raw_for_log}'")
        print(f"   Processed query (after decode): '{query}'")
    else:
        query_raw_for_log = data.get("query", "")
        print(f"   Raw query (from JSON): '{query_raw_for_log}'")
        print(f"   Processed query: '{query}'")
    print(f"   Query length: {len(query)}, bytes: {len(query.encode('utf-8'))}")
    print(f"   Query repr: {repr(query)}")  # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß k√Ω t·ª± ƒë·∫∑c bi·ªát
    print(f"   top_k_topics={top_k_topics}, top_k_docs_per_topic={top_k_docs_per_topic}, sim_threshold={sim_threshold}")
    
    try:
        res = search_core(
            query=query,
            top_k_topics=top_k_topics,
            top_k_docs_per_topic=top_k_docs_per_topic,
            sim_threshold=sim_threshold,
        )
        print(f"‚úÖ Search completed: {len(res.get('results', []))} topics found")
        return jsonify(res)
    except Exception as e:
        print(f"‚ùå Error in semantic search: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "query": query,
            "error": str(e),
            "results": []
        }), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5005)
